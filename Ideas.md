Note: "the NER tools" refers to the 4 "off-the-shelf NER tools" described in \cite{dekker2019evaluating}

* How have the 4 tools evolved over the last 3 years?

* Compare the performance of the tools on modern novels/new fiction novels with the one on classical novels
(Take texts from different genres and compare how NER tools perform on those)

* Pick individual texts that have a strong representation of individual challenges (e.g. D'Artagnan, personification, etc.) or generate such texts, and compare how the NER tools perform

* How well do the NER tools identify fictional characters with non-traditional names in novels?

* Compare the performance of the tools on novels with that on (newspaper, blog, fake) articles, historical letters, and short texts (messages, tweets). How does the performance change? 

* Replace names that might be mapped by datasets (e.g. lists of common names, DBpedia) and compare performance - targeting tools depending on mapping to external sources, rather than the text as such.

* How well do general solutions/libraries (e.g. spaCy, textacy, fastText) handle novels vs. articles

* Compare to three different general tools that cover the categories - rule-based, learning-based, hybrid

* How well do the general tools perform when no external data sources can support the recognition (e.g. cannot map entities to DBpedia).



### Secondary ideas
* Comparing issues of NER tools with English and German text? - similarities and differences
